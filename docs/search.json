[
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "projects/churn.html#about-the-data",
    "href": "projects/churn.html#about-the-data",
    "title": "Churn Analytics and Prediction",
    "section": "About the Data",
    "text": "About the Data"
  },
  {
    "objectID": "projects/churn.html#exploratory-data-analysis",
    "href": "projects/churn.html#exploratory-data-analysis",
    "title": "Churn Analytics and Prediction",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Thanks for checking out my web site!"
  },
  {
    "objectID": "projects/Livestock/livestock.html",
    "href": "projects/Livestock/livestock.html",
    "title": "Livestock Pattern Analysis",
    "section": "",
    "text": "We are going to analyze the Livestock data downloaded from Food and Agriculture Organization of the United Nations website. Below are the questions we are going to answer by using our data.\n\nWhat Items or Livestock has the highest density for the most recent year by:\n\nOverall (World wide)\nRegion\n\nWhat is the trend of total Livestock density for:\n\nOverall\nRegion\n\nIdentify if there is a significant difference in Livestock Density for each Region.\nDetermine the Top 10 country with the Highest Livestock Density.\nCreate a Time Series Model for:\n\nThe top 3 Livestock Product of the Top 1 Country by Livestock Density for each Region"
  },
  {
    "objectID": "projects/Livestock/livestock.html#overview",
    "href": "projects/Livestock/livestock.html#overview",
    "title": "Livestock Pattern Analysis",
    "section": "",
    "text": "We are going to analyze the Livestock data downloaded from Food and Agriculture Organization of the United Nations website. Below are the questions we are going to answer by using our data.\n\nWhat Items or Livestock has the highest density for the most recent year by:\n\nOverall (World wide)\nRegion\n\nWhat is the trend of total Livestock density for:\n\nOverall\nRegion\n\nIdentify if there is a significant difference in Livestock Density for each Region.\nDetermine the Top 10 country with the Highest Livestock Density.\nCreate a Time Series Model for:\n\nThe top 3 Livestock Product of the Top 1 Country by Livestock Density for each Region"
  },
  {
    "objectID": "projects/Livestock/livestock.html#data-import-and-pre-process",
    "href": "projects/Livestock/livestock.html#data-import-and-pre-process",
    "title": "Livestock Pattern Analysis",
    "section": "Data Import and Pre-Process",
    "text": "Data Import and Pre-Process\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(countrycode)\nlibrary(FSA)\nlibrary(forecast)\n\nlivestock_data = fread(\"Livestock_data.csv\") %&gt;% as_tibble()\n\n\n#Check Dimension of Data\nglimpse(livestock_data)\n\nRows: 8,180\nColumns: 70\n$ `Area Code`       &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ `Area Code (M49)` &lt;chr&gt; \"'004\", \"'004\", \"'004\", \"'004\", \"'004\", \"'004\", \"'00…\n$ Area              &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghan…\n$ `Item Code`       &lt;int&gt; 1107, 1107, 1107, 1126, 1126, 1126, 866, 866, 866, 1…\n$ `Item Code (CPC)` &lt;chr&gt; \"'02132\", \"'02132\", \"'02132\", \"'02121.01\", \"'02121.0…\n$ Item              &lt;chr&gt; \"Asses\", \"Asses\", \"Asses\", \"Camels\", \"Camels\", \"Came…\n$ `Element Code`    &lt;int&gt; 7213, 7211, 5118, 7213, 7211, 5118, 7213, 7211, 5118…\n$ Element           &lt;chr&gt; \"Livestock units per agricultural land area\", \"Share…\n$ Unit              &lt;chr&gt; \"LSU/ha\", \"%LSU\", \"LSU\", \"LSU/ha\", \"%LSU\", \"LSU\", \"L…\n$ Y1961             &lt;dbl&gt; 0.02, 12.36, 650000.00, 0.00, 3.57, 187500.00, 0.05,…\n$ Y1962             &lt;dbl&gt; 0.01, 8.10, 425925.00, 0.01, 3.99, 209932.50, 0.06, …\n$ Y1963             &lt;dbl&gt; 0.01, 9.15, 500556.00, 0.01, 4.79, 262053.75, 0.06, …\n$ Y1964             &lt;dbl&gt; 0.02, 10.27, 575000.00, 0.01, 4.35, 243750.00, 0.06,…\n$ Y1965             &lt;dbl&gt; 2.000e-02, 1.134e+01, 6.500e+05, 1.000e-02, 3.920e+0…\n$ Y1966             &lt;dbl&gt; 2.000e-02, 1.016e+01, 6.000e+05, 1.000e-02, 3.810e+0…\n$ Y1967             &lt;dbl&gt; 2.000e-02, 1.014e+01, 6.000e+05, 1.000e-02, 3.800e+0…\n$ Y1968             &lt;dbl&gt; 0.02, 10.84, 664000.00, 0.01, 3.66, 224250.00, 0.07,…\n$ Y1969             &lt;dbl&gt; 2.000e-02, 1.033e+01, 6.250e+05, 1.000e-02, 3.720e+0…\n$ Y1970             &lt;dbl&gt; 2.000e-02, 1.051e+01, 6.500e+05, 1.000e-02, 3.640e+0…\n$ Y1971             &lt;dbl&gt; 2.000e-02, 1.057e+01, 6.500e+05, 1.000e-02, 3.660e+0…\n$ Y1972             &lt;dbl&gt; 2.000e-02, 1.309e+01, 6.500e+05, 1.000e-02, 4.530e+0…\n$ Y1973             &lt;dbl&gt; 0.02, 12.06, 625000.00, 0.01, 4.34, 225000.00, 0.06,…\n$ Y1974             &lt;dbl&gt; 0.02, 11.26, 625000.00, 0.01, 4.05, 225000.00, 0.06,…\n$ Y1975             &lt;dbl&gt; 0.02, 10.48, 625000.00, 0.01, 3.77, 225000.00, 0.07,…\n$ Y1976             &lt;dbl&gt; 2.000e-02, 1.027e+01, 6.250e+05, 1.000e-02, 3.700e+0…\n$ Y1977             &lt;dbl&gt; 0.02, 10.84, 650000.00, 0.01, 3.75, 225000.00, 0.07,…\n$ Y1978             &lt;dbl&gt; 0.02, 10.95, 650000.00, 0.01, 3.79, 225000.00, 0.07,…\n$ Y1979             &lt;dbl&gt; 2.000e-02, 1.117e+01, 6.500e+05, 1.000e-02, 3.480e+0…\n$ Y1980             &lt;dbl&gt; 0.02, 11.08, 647500.00, 0.01, 3.40, 198750.00, 0.07,…\n$ Y1981             &lt;dbl&gt; 0.02, 11.13, 657500.00, 0.01, 3.36, 198750.00, 0.07,…\n$ Y1982             &lt;dbl&gt; 0.02, 11.14, 657500.00, 0.01, 3.37, 198750.00, 0.07,…\n$ Y1983             &lt;dbl&gt; 0.02, 11.93, 657500.00, 0.01, 3.60, 198750.00, 0.06,…\n$ Y1984             &lt;dbl&gt; 0.02, 13.91, 659000.00, 0.01, 4.20, 198750.00, 0.05,…\n$ Y1985             &lt;dbl&gt; 0.02, 15.60, 660500.00, 0.01, 4.69, 198750.00, 0.04,…\n$ Y1986             &lt;dbl&gt; 0.02, 19.31, 662500.00, 0.01, 5.79, 198750.00, 0.03,…\n$ Y1987             &lt;dbl&gt; 2.000e-02, 1.777e+01, 6.500e+05, 1.000e-02, 6.150e+0…\n$ Y1988             &lt;dbl&gt; 0.01, 13.37, 500000.00, 0.01, 5.32, 198750.00, 0.03,…\n$ Y1989             &lt;dbl&gt; 0.01, 10.98, 400000.00, 0.00, 4.84, 176250.00, 0.03,…\n$ Y1990             &lt;dbl&gt; 0.01, 8.43, 300000.00, 0.00, 4.53, 161250.00, 0.03, …\n$ Y1991             &lt;dbl&gt; 1.000e-02, 7.080e+00, 2.500e+05, 0.000e+00, 4.250e+0…\n$ Y1992             &lt;dbl&gt; 1.000e-02, 7.060e+00, 2.500e+05, 0.000e+00, 4.230e+0…\n$ Y1993             &lt;dbl&gt; 1.000e-02, 7.030e+00, 2.500e+05, 0.000e+00, 4.220e+0…\n$ Y1994             &lt;dbl&gt; 1.000e-02, 8.170e+00, 3.000e+05, 0.000e+00, 4.090e+0…\n$ Y1995             &lt;dbl&gt; 0.01, 9.06, 352000.00, 0.00, 3.88, 150750.00, 0.04, …\n$ Y1996             &lt;dbl&gt; 0.01, 8.59, 376500.00, 0.00, 3.78, 165750.00, 0.05, …\n$ Y1997             &lt;dbl&gt; 0.01, 8.37, 402500.00, 0.00, 3.77, 181500.00, 0.05, …\n$ Y1998             &lt;dbl&gt; 0.01, 8.34, 430000.00, 0.01, 3.86, 198750.00, 0.06, …\n$ Y1999             &lt;dbl&gt; 0.01, 8.00, 459970.00, 0.01, 3.79, 217788.00, 0.06, …\n$ Y2000             &lt;dbl&gt; 1.000e-02, 6.930e+00, 3.410e+05, 0.000e+00, 3.410e+0…\n$ Y2001             &lt;dbl&gt; 0.01, 8.26, 341000.00, 0.00, 4.07, 168000.00, 0.04, …\n$ Y2002             &lt;dbl&gt; 0.02, 14.91, 794000.00, 0.00, 2.47, 131250.00, 0.07,…\n$ Y2003             &lt;dbl&gt; 0.02, 14.63, 799000.00, 0.00, 2.49, 135750.00, 0.07,…\n$ Y2004             &lt;dbl&gt; 0.02, 14.99, 807000.00, 0.00, 2.65, 142500.00, 0.06,…\n$ Y2005             &lt;dbl&gt; 0.02, 12.79, 695500.00, 0.00, 2.59, 141000.00, 0.07,…\n$ Y2006             &lt;dbl&gt; 2.000e-02, 1.126e+01, 6.075e+05, 0.000e+00, 2.420e+0…\n$ Y2007             &lt;dbl&gt; 0.02, 13.54, 736000.00, 0.00, 2.57, 139500.00, 0.08,…\n$ Y2008             &lt;dbl&gt; 0.02, 10.14, 604500.00, 0.00, 2.30, 137250.00, 0.09,…\n$ Y2009             &lt;dbl&gt; 0.02, 10.83, 661000.00, 0.00, 2.33, 142500.00, 0.09,…\n$ Y2010             &lt;dbl&gt; 0.02, 9.97, 702500.00, 0.00, 2.03, 143250.00, 0.10, …\n$ Y2011             &lt;dbl&gt; 0.02, 10.27, 733000.00, 0.00, 1.81, 129000.00, 0.10,…\n$ Y2012             &lt;dbl&gt; 0.02, 10.40, 711500.00, 0.00, 1.91, 130500.00, 0.10,…\n$ Y2013             &lt;dbl&gt; 0.02, 10.77, 725500.00, 0.00, 1.89, 127500.00, 0.10,…\n$ Y2014             &lt;dbl&gt; 0.02, 10.53, 720500.00, 0.00, 1.87, 128250.00, 0.10,…\n$ Y2015             &lt;dbl&gt; 0.02, 10.81, 740500.00, 0.00, 1.86, 127500.00, 0.10,…\n$ Y2016             &lt;dbl&gt; 0.02, 10.82, 736050.00, 0.00, 1.88, 127875.00, 0.10,…\n$ Y2017             &lt;dbl&gt; 0.02, 9.92, 658500.00, 0.00, 1.94, 129000.00, 0.09, …\n$ Y2018             &lt;dbl&gt; 0.02, 10.01, 682038.97, 0.00, 1.89, 129068.99, 0.09,…\n$ Y2019             &lt;dbl&gt; 0.02, 11.44, 781841.65, 0.00, 1.87, 127661.06, 0.09,…\n$ Y2020             &lt;dbl&gt; 0.02, 11.37, 771299.07, 0.00, 1.88, 127474.37, 0.09,…\n$ Y2021             &lt;dbl&gt; 0.02, 11.47, 780882.17, 0.00, 1.87, 126964.23, 0.09,…\n\n#8180 rows and 70 Columns\n\n\n\n\n\n\n\nNote\n\n\n\nData Dictionary\n\nArea Code = Code for the Country\nArea Code (M49) = Standard Country or Area Code for Statistical Use\nArea = Country Name\nItem Code = Code for Livestock Product/Item\nItem Code (CPC)= Customs Procedure Code to determine if product is for Import or Export\nItem = Name of livestock product\nElement = Unit of measurement for Livestock\nElement Code = Numeric code for Livestock Unit of Measurement\nUnit = Shorthand notation or symbol of Element Column\nY&lt;9999&gt; = Year\n\n\n\nWe need to add a region column for us to answer some of the questions on our list above.\n\nlivestock_data = livestock_data %&gt;% \n  mutate(\n    Region = countrycode(\n      sourcevar = Area,\n      origin = \"country.name\",\n      destination = \"continent\"\n    )\n  ) %&gt;% \n  filter(\n    !is.na(Region)\n  )\n\nWarning: There were 737 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `Region = countrycode(sourcevar = Area, origin = \"country.name\",\n  destination = \"continent\")`.\nCaused by warning in `grepl()`:\n! input string 1 is invalid UTF-8\nℹ Run `dplyr::last_dplyr_warnings()` to see the 736 remaining warnings.\n\nhead(livestock_data)\n\n# A tibble: 6 × 71\n  `Area Code` `Area Code (M49)` Area        `Item Code` `Item Code (CPC)` Item  \n        &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;             &lt;int&gt; &lt;chr&gt;             &lt;chr&gt; \n1           2 '004              Afghanistan        1107 '02132            Asses \n2           2 '004              Afghanistan        1107 '02132            Asses \n3           2 '004              Afghanistan        1107 '02132            Asses \n4           2 '004              Afghanistan        1126 '02121.01         Camels\n5           2 '004              Afghanistan        1126 '02121.01         Camels\n6           2 '004              Afghanistan        1126 '02121.01         Camels\n# ℹ 65 more variables: `Element Code` &lt;int&gt;, Element &lt;chr&gt;, Unit &lt;chr&gt;,\n#   Y1961 &lt;dbl&gt;, Y1962 &lt;dbl&gt;, Y1963 &lt;dbl&gt;, Y1964 &lt;dbl&gt;, Y1965 &lt;dbl&gt;,\n#   Y1966 &lt;dbl&gt;, Y1967 &lt;dbl&gt;, Y1968 &lt;dbl&gt;, Y1969 &lt;dbl&gt;, Y1970 &lt;dbl&gt;,\n#   Y1971 &lt;dbl&gt;, Y1972 &lt;dbl&gt;, Y1973 &lt;dbl&gt;, Y1974 &lt;dbl&gt;, Y1975 &lt;dbl&gt;,\n#   Y1976 &lt;dbl&gt;, Y1977 &lt;dbl&gt;, Y1978 &lt;dbl&gt;, Y1979 &lt;dbl&gt;, Y1980 &lt;dbl&gt;,\n#   Y1981 &lt;dbl&gt;, Y1982 &lt;dbl&gt;, Y1983 &lt;dbl&gt;, Y1984 &lt;dbl&gt;, Y1985 &lt;dbl&gt;,\n#   Y1986 &lt;dbl&gt;, Y1987 &lt;dbl&gt;, Y1988 &lt;dbl&gt;, Y1989 &lt;dbl&gt;, Y1990 &lt;dbl&gt;, …\n\n\nNow that we have our Region/Continent column we can start answering our questions from the list above"
  },
  {
    "objectID": "projects/Livestock/livestock.html#questions",
    "href": "projects/Livestock/livestock.html#questions",
    "title": "Livestock Pattern Analysis",
    "section": "Questions",
    "text": "Questions\n\nHighest Livestock Density\n\nOverall\n\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  group_by(Item) %&gt;% \n  reframe(\n    Livestock_Density = sum(Y2021, na.rm = TRUE)\n  ) %&gt;% \n  arrange(desc(Livestock_Density)) %&gt;% \n  top_n(10) %&gt;% \n  mutate(\n    Item = reorder(Item, Livestock_Density)\n  ) %&gt;% \n  ggplot(aes(x = Item, y = Livestock_Density)) +\n  geom_col()+\n  coord_flip()\n\n\n\n\n\nRegion\n\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  group_by(Region, Item) %&gt;% \n  reframe(\n    Livestock_Density = sum(Y2021, na.rm = TRUE)\n  ) %&gt;% \n  arrange(desc(Livestock_Density)) %&gt;% \n  #top_n(10) %&gt;% \n  mutate(\n    Region = reorder(Region, Livestock_Density)\n  ) %&gt;% \n  ggplot(aes(x = Region, y = Livestock_Density, fill = Item)) +\n  geom_col()+\n  coord_flip()\n\n\n\n\n\n\nLivestock Density Trend\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  pivot_longer(-c(1:9,71),names_to = \"Year\", values_to = \"Livestock_Density\") %&gt;% \n  group_by(Year) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density,na.rm = TRUE)\n  ) %&gt;% \n  mutate(\n    Year = str_remove(Year, \"Y\"),\n    Year = paste(Year,\"01\",\"01\",sep = \"-\"),\n    Year = as.Date(Year)\n  ) %&gt;% \n  ggplot(aes(x = Year, Livestock_Density,group = 1)) +\n  geom_point()+\n  geom_line()\n\n\n\n\nInteresting. There is a sharp dip of Total Livestock Density in Worldwide Level. Let’s investigate this further\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  pivot_longer(-c(1:9,71),names_to = \"Year\", values_to = \"Livestock_Density\") %&gt;% \n  group_by(Year) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density,na.rm = TRUE)\n  ) %&gt;% \n  mutate(\n    Year = str_remove(Year, \"Y\"),\n    Year = paste(Year,\"01\",\"01\",sep = \"-\"),\n    Year = as.Date(Year)\n  ) %&gt;% \n  slice(which.min(Livestock_Density))\n\n# A tibble: 1 × 2\n  Year       Livestock_Density\n  &lt;date&gt;                 &lt;dbl&gt;\n1 1961-01-01              248.\n\n\nThe year where we saw the sharp dip is 1961. Let’s see what happen on this year by splicing the Livestock computation by region. Maybe we can see a region dropping its Livestock, or maybe we can see that all region will have a sharp dip.\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  pivot_longer(-c(1:9,71),names_to = \"Year\", values_to = \"Livestock_Density\") %&gt;% \n  group_by(Region,Year) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density,na.rm = TRUE)\n  ) %&gt;% \n  mutate(\n    Year = str_remove(Year, \"Y\"),\n    Year = paste(Year,\"01\",\"01\",sep = \"-\"),\n    Year = as.Date(Year)\n  ) %&gt;% \n  ggplot(aes(x = Year, Livestock_Density,group = Region, color = Region)) +\n  geom_point()+\n  geom_line()\n\n\n\n\nAsia seems to be the driver of that sharp dip we saw from the world level. Lets try to remove Asia from the graph. Looking at the visual, there seems to be a drop on other continent as well. The scale of Asia’s Livestock density is making the trend on other continents seem small.\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  pivot_longer(-c(1:9,71),names_to = \"Year\", values_to = \"Livestock_Density\") %&gt;% \n  filter(Region != \"Asia\") %&gt;% \n  group_by(Region,Year) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density,na.rm = TRUE)\n  ) %&gt;% \n  mutate(\n    Year = str_remove(Year, \"Y\"),\n    Year = paste(Year,\"01\",\"01\",sep = \"-\"),\n    Year = as.Date(Year)\n  ) %&gt;% \n  ggplot(aes(x = Year, Livestock_Density,group = Region, color = Region)) +\n  geom_point()+\n  geom_line()\n\n\n\n\nLooks like Asia is really the one causing the dip. Let’s try to look at this one level lower.\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  pivot_longer(-c(1:9,71),names_to = \"Year\", values_to = \"Livestock_Density\") %&gt;% \n  filter(Region == \"Asia\") %&gt;% \n  group_by(Area,Year) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density,na.rm = TRUE)\n  ) %&gt;% \n  mutate(\n    Year = str_remove(Year, \"Y\"),\n    Year = paste(Year,\"01\",\"01\",sep = \"-\"),\n    Year = as.Date(Year)\n  ) %&gt;% \n  ggplot(aes(x = Year, Livestock_Density,group = Area, color = Area)) +\n  geom_point(show.legend = FALSE)+\n  geom_line(show.legend = FALSE)\n\n\n\n\nIt is hard to identify what country is displaying the sharp dip for 1991. Lets do it using dplyr\n\ndip_country = livestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  pivot_longer(-c(1:9,71),names_to = \"Year\", values_to = \"Livestock_Density\") %&gt;% \n  filter(Region == \"Asia\") %&gt;% \n  group_by(Area,Year) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density,na.rm = TRUE)\n  ) %&gt;% \n  mutate(\n    Year = str_remove(Year, \"Y\"),\n    Year = paste(Year,\"01\",\"01\",sep = \"-\"),\n    Year = as.Date(Year)\n  ) \n\ndip_country %&gt;% \n  mutate(\n    LagDiff = c(0,diff(dip_country$Livestock_Density))\n  ) %&gt;% \n  filter(LagDiff &lt; 0) %&gt;% \n  filter(LagDiff &lt; -15)\n\n# A tibble: 5 × 4\n  Area                      Year       Livestock_Density LagDiff\n  &lt;chr&gt;                     &lt;date&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n1 Cambodia                  1961-01-01              1.23   -31.5\n2 China, Taiwan Province of 1961-01-01              2.98   -34.2\n3 Singapore                 1988-01-01             69.2    -29.2\n4 Singapore                 1991-01-01             34.1    -62.3\n5 Sri Lanka                 1961-01-01              2.14  -117. \n\n\nWe created a new column called LagDiff which is the result of calculating the difference of the current Livestock Density and its previous value. That way, we can see what year and country have the highest negative LagDiff that will indicate a sharp dip.\nLooking at the result, we see that its probably Singapore. Since Cambodia, China and Sri Lanka’s value are all from 1961 which is obviously not the year of Interest for us.\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  pivot_longer(-c(1:9,71),names_to = \"Year\", values_to = \"Livestock_Density\") %&gt;% \n  filter(Region == \"Asia\") %&gt;% \n  filter(Area == \"Singapore\") %&gt;% \n  group_by(Area,Year) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density,na.rm = TRUE)\n  ) %&gt;% \n  mutate(\n    Year = str_remove(Year, \"Y\"),\n    Year = paste(Year,\"01\",\"01\",sep = \"-\"),\n    Year = as.Date(Year)\n  ) %&gt;% \n  ggplot(aes(x = Year, Livestock_Density,group = Area, color = Area)) +\n  geom_point(show.legend = FALSE)+\n  geom_line(show.legend = FALSE)\n\n\n\n\nSingapore is the country influencing the dip at year 1991. Doing a bit of research about this sharp drop, it turns out that the cause of this drop is due to Mt. Pinatubo’s volcanic eruption in the Philippines. Lets add the Philippines on our chart\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  pivot_longer(-c(1:9,71),names_to = \"Year\", values_to = \"Livestock_Density\") %&gt;% \n  filter(Region == \"Asia\") %&gt;% \n  filter(Area %in% c(\"Singapore\",\"Philippines\") ) %&gt;% \n  group_by(Area,Year) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density,na.rm = TRUE)\n  ) %&gt;% \n  mutate(\n    Year = str_remove(Year, \"Y\"),\n    Year = paste(Year,\"01\",\"01\",sep = \"-\"),\n    Year = as.Date(Year)\n  ) %&gt;% \n  ggplot(aes(x = Year, Livestock_Density,group = Area, color = Area)) +\n  geom_point()+\n  geom_line()\n\n\n\n\nSingapore’s Livestock Density value is too high for us to see the variance in Ph’s data.\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  pivot_longer(-c(1:9,71),names_to = \"Year\", values_to = \"Livestock_Density\") %&gt;% \n  filter(Region == \"Asia\") %&gt;% \n  filter(Area %in% c(\"Philippines\") ) %&gt;% \n  group_by(Area,Year) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density,na.rm = TRUE)\n  ) %&gt;% \n  mutate(\n    Year = str_remove(Year, \"Y\"),\n    Year = paste(Year,\"01\",\"01\",sep = \"-\"),\n    Year = as.Date(Year)\n  ) %&gt;% \n  ggplot(aes(x = Year, Livestock_Density,group = Area, color = Area)) +\n  geom_point()+\n  geom_line()\n\n\n\n\nLooks like even before that year, the livestock density for Ph is already low, that’s why the decrease in livestock when Mt. Pinatubo erupted does not really cause much of change in Ph’s Livestock Density.\n\n\nRegion Significant Difference\n\nlivestock_by_region = livestock_data %&gt;% \n  group_by(Region) %&gt;% \n  reframe(\n    Livestock_Density = sum(Y2021, na.rm = TRUE)\n  )\n\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  ggplot(aes(x = Region, y = Y2021)) +\n  geom_boxplot()\n\n\n\n\nBefore we can use ANOVA check if there is a significant difference on Livestock Density for each Region, we need to determine first if the data on each group is normal.\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  filter(Region == \"Asia\") %&gt;% \n  filter(!is.na(Y2021)) %&gt;% \n  pull(Y2021) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.11448, p-value &lt; 2.2e-16\n\n\nAsia’s value for Livestock Density is not normal since we have a p.value of less than 0.05\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  filter(Region == \"Africa\") %&gt;% \n  filter(!is.na(Y2021)) %&gt;% \n  pull(Y2021) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.502, p-value &lt; 2.2e-16\n\n\nSame for Africa.\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  filter(Region == \"Europe\") %&gt;% \n  filter(!is.na(Y2021)) %&gt;% \n  pull(Y2021) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.50173, p-value &lt; 2.2e-16\n\n\nEurope is also not normal. At this point, we dont need to check the other 2 Region, since as pre ANOVA’s requirement, all group should come from a normal distribution. We can still determine if there is a significant difference among the groups by using a Non-Parametric approach. This approach will not require our data to come from a normal distribution.\n\nlivestock_region_data = livestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  filter(!is.na(Y2021)) %&gt;% \n  select(Region, Y2021)\n\nlivestock_kruskal = kruskal.test(Y2021 ~ Region, data = livestock_region_data )\nlivestock_kruskal\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Y2021 by Region\nKruskal-Wallis chi-squared = 35.535, df = 4, p-value = 3.606e-07\n\n\nWe have a p.value of less than 0.05 indicating that we have a significant difference in Livestock Density for each Region. But let us take the test further and identify which among these groups have a significant difference among each other. We will use Dunn’s test for this task.\n\ndunn_result = dunnTest(Y2021 ~ Region, data = livestock_region_data)\n\n#Let's filter out those comparison test with an adjusted p.value of greater than 0.05\n\n\ndunn_result$res %&gt;% \n  filter(P.adj &lt; 0.05)\n\n         Comparison         Z      P.unadj        P.adj\n1 Africa - Americas -4.484740 7.300273e-06 6.570246e-05\n2     Africa - Asia -4.106799 4.011807e-05 3.209446e-04\n3   Africa - Europe -3.235075 1.216108e-03 8.512756e-03\n4  Africa - Oceania -4.631136 3.636652e-06 3.636652e-05\n\n\nChecking the result from Dunn’s test, it seems that Africa have a significant difference on Livestock Desnity with all other Region in our data set.\n\n\nTop 10 Country with Highest Livestock Density\nWe can easily extract this information with a simple code using dplyr.\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  select(Area, Y2021) %&gt;% \n  filter(!is.na(Y2021)) %&gt;% \n  group_by(Area) %&gt;% \n  reframe(\n    Livestock_Density = sum(Y2021, na.rm = TRUE)\n  ) %&gt;% \n  arrange(desc(Livestock_Density)) %&gt;% \n  top_n(10)\n\n# A tibble: 10 × 2\n   Area                             Livestock_Density\n   &lt;chr&gt;                                        &lt;dbl&gt;\n 1 Singapore                                   119.  \n 2 China, Hong Kong SAR                         37.2 \n 3 Brunei Darussalam                            32.7 \n 4 Trinidad and Tobago                          16.4 \n 5 Barbados                                     13.1 \n 6 Micronesia (Federated States of)             10.8 \n 7 Republic of Korea                            10.7 \n 8 Kuwait                                       10.2 \n 9 Qatar                                        10.2 \n10 Netherlands (Kingdom of the)                  9.95\n\n\nWe can also check the top 10 for each Region and visualize it for better view.\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  filter(!is.na(Y2021)) %&gt;% \n  group_by(Region, Area) %&gt;% \n  summarise(\n    Livestock_Density = sum(Y2021, na.rm = TRUE)\n  ) %&gt;% \n  top_n(10) %&gt;% \n  ungroup() %&gt;% \n  mutate(\n    Area = reorder(Area, Livestock_Density)\n  ) %&gt;% \n  ggplot(aes(x = Area, y = Livestock_Density, fill = Region)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~Region, scales = \"free\") +\n  scale_x_discrete(label = function(x) abbreviate(x, minlength = 7)) +\n  coord_flip()\n\n\n\n\n\nlivestock_data %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  filter(!is.na(Y2021)) %&gt;% \n  group_by(Region, Area) %&gt;% \n  summarise(\n    Livestock_Density = sum(Y2021, na.rm = TRUE)\n  ) %&gt;% \n  top_n(10) %&gt;% \n  filter(Region == \"Africa\") %&gt;% \n  arrange(desc(Livestock_Density))\n\n`summarise()` has grouped output by 'Region'. You can override using the\n`.groups` argument.\nSelecting by Livestock_Density\n\n\n# A tibble: 10 × 3\n# Groups:   Region [1]\n   Region Area                     Livestock_Density\n   &lt;chr&gt;  &lt;chr&gt;                                &lt;dbl&gt;\n 1 Africa Seychelles                            4.3 \n 2 Africa Mauritius                             4.13\n 3 Africa Ethiopia                              3.68\n 4 Africa Egypt                                 3.67\n 5 Africa Burkina Faso                          2.13\n 6 Africa Kenya                                 2.13\n 7 Africa Guinea-Bissau                         2.1 \n 8 Africa Uganda                                2.04\n 9 Africa Cabo Verde                            1.98\n10 Africa Central African Republic              1.97\n\n\n\n\nTime Series Modelling\nLet’s start by creating a time series model for Africa’s highest Livestock Density Country, Seychelles\n\n#Let's identify first the top 3 Livestock Product for Seychelles\nlivestock_data %&gt;% \n  filter(Region == \"Africa\") %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  filter(Area == \"Seychelles\") %&gt;% \n  group_by(Item) %&gt;% \n  reframe(\n    Livestock_Density = sum(Y2021, na.rm = TRUE)\n  ) %&gt;% \n  arrange(desc(Livestock_Density))\n\n# A tibble: 7 × 2\n  Item                  Livestock_Density\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 Major livestock types              1.92\n2 Chickens                           0.84\n3 Swine / pigs                       0.62\n4 Goats                              0.37\n5 Sheep and Goats                    0.37\n6 Cattle                             0.09\n7 Cattle and Buffaloes               0.09\n\n#Top 3 Livestock Item for Seychelles is Major Livestock Types, Chickens and Swine/Pigs\n\n\n#Lets Filter the Major Livestock Types first\nafrica_1 = livestock_data %&gt;% \n  filter(Region == \"Africa\") %&gt;% \n  filter(Element == \"Livestock units per agricultural land area\") %&gt;% \n  filter(Area == \"Seychelles\") %&gt;% \n  select(Area,Item,c(10:70)) %&gt;% \n  pivot_longer(-c(Area,Item),names_to = \"Year\", values_to = \"Livestock_Density\") %&gt;% \n   mutate(\n    Year = str_remove(Year, \"Y\"),\n    Year = paste(Year,\"01\",\"01\",sep = \"-\"),\n    Year = as.Date(Year)\n  ) %&gt;% \n  group_by(Area,Item, Year) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density, na.rm = TRUE)\n  )\n\n\nafrica_1 %&gt;% \n  group_by(Item) %&gt;% \n  reframe(\n    Livestock_Density = sum(Livestock_Density,na.rm = TRUE)\n  ) %&gt;% \n  arrange(desc(Livestock_Density))\n\n# A tibble: 7 × 2\n  Item                  Livestock_Density\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 Major livestock types             91.2 \n2 Chickens                          41.4 \n3 Swine / pigs                      30.5 \n4 Cattle                            10.9 \n5 Cattle and Buffaloes              10.9 \n6 Goats                              8.52\n7 Sheep and Goats                    8.52\n\n\n\nafrica_1 %&gt;% \n  filter(Item %in% c(\"Major livestock types\",\"Chickens\",\"Swine / pigs\")) %&gt;% \n  ggplot(aes(x = Year,y = Livestock_Density, color = Item, group = Item)) +\n  geom_point() +\n  geom_line()\n\n\n\n\nWe now know what are the top 3 Livestock Item for Seychelles in Africa from 1960 to 2021. We will then create a time series model for each livestock Item and compare the ratio of these 3 throughout each time period.\n\n#Lets create a time series object first\nlivestock_ts_major = africa_1 %&gt;% \n  filter(Item == \"Major livestock types\") %&gt;% \n  select(Year, Livestock_Density)\n\nlivestock_ts_obj = ts(livestock_ts_major$Livestock_Density, start = 1961, frequency = 1)\n\nTransform our Time Series data to Log Form to remove fluctuations\n\nlog_ls_ts = log(livestock_ts_obj)\nplot(log_ls_ts)\n\n\n\n\nWe can see that there is an upward trend for our Time Series Data.\nFor creating the Time Series Model, we will use the function auto.arima. This will help us to identify the best configuration for our ARIMA model.\n\narima_model = auto.arima(log_ls_ts)\narima_model\n\nSeries: log_ls_ts \nARIMA(2,1,2) with drift \n\nCoefficients:\n         ar1      ar2      ma1     ma2   drift\n      0.2600  -0.7282  -0.4762  0.5582  0.0281\ns.e.  0.2804   0.1492   0.3711  0.1959  0.0130\n\nsigma^2 = 0.02032:  log likelihood = 34.13\nAIC=-56.27   AICc=-54.68   BIC=-43.7\n\n\nNow that we are done with the Model, let’s explore our ACF and PACF plots\n\nacf(arima_model$residuals, main = \"Correlogram\")\n\n\n\n\n\npacf(arima_model$residuals, main = \"Partial Correlogram\")\n\n\n\n\nTesting Significance of Residuals using Ljung-Box Test\n\nBox.test(arima_model$residuals, lag = 20, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  arima_model$residuals\nX-squared = 8.9967, df = 20, p-value = 0.9829\n\n\nSince our pvalue is greater than 0.05, we can conclude that there is no evidence that there is a Non-Zero Autocorrelation in our forecast error from Lags 1 to 20.\nNext we will generate a Residual Plot. This will help us confirm if our model is correct or if we need to consider other modelling algorithm.\n\nhist(arima_model$residuals,\n     col = \"red\",\n     xlab = \"Error\",\n     main = \"Histogram of Residuals\",\n     freq = FALSE)\n\n\n\n\nAs we can see, most of the values for Error are concentrated on 0.\nFinally, we can now start to forecast using our ARIMA model. On our script below we will try to predict the Major Livestock Density for the next 5 years\n\nmodel_forecast = forecast(arima_model, 5)\nautoplot(model_forecast)\n\n\n\n\nOur model captured the upward trend of our Log Transformed Time Series Data. The dark and light blue region indicates the confidence interval of our prediction or forecast. Notice that our model did not capture any fluctuation from our original data, that is because our data does not have a seasonality component. The up and down movement we are seeing may be due to noise but not enough for our model to consider it as a pattern for forecasting.\n\naccuracy(model_forecast)\n\n                       ME      RMSE        MAE       MPE     MAPE      MASE\nTraining set -0.000373242 0.1353569 0.09882457 -3.725761 33.46294 0.9112668\n                   ACF1\nTraining set 0.02882789\n\n\nRunning accuracy function tells us how well our model is performing. The rmse value tells us the root mean squared error of our model. Our target for this metric is for it to be lower when compared to other model performance evaluation."
  },
  {
    "objectID": "projects/telecom_churn/churn_telecom.html",
    "href": "projects/telecom_churn/churn_telecom.html",
    "title": "Telecom Customer Churn Analysis",
    "section": "",
    "text": "Churn is the percentage of customers that stop using a service or product over a given period of time. It is an important metric that most of the company is tracking. Analyzing churn will help the organization to identify how huge their churn rate is, what is causing the churn and how can they prevent it from happening.\nOn this project, we will use Telecom Customer Churn data to answer the following questions:\nAfter answering Exploratory Questions we will proceed to create 3 Classification Model that will predict Churn. We will compare each model and decide which model to use for predicting whether a customer will churn or no."
  },
  {
    "objectID": "projects/telecom_churn/churn_telecom.html#library-and-data-import",
    "href": "projects/telecom_churn/churn_telecom.html#library-and-data-import",
    "title": "Telecom Customer Churn Analysis",
    "section": "Library and Data Import",
    "text": "Library and Data Import\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(doParallel)\n\nall_cores = parallel::detectCores(logical = FALSE)\ndoParallel::registerDoParallel(cores = all_cores)\n\nchurn_data = fread(\"churn_data.csv\") %&gt;% as_tibble()"
  },
  {
    "objectID": "projects/telecom_churn/churn_telecom.html#exploratory-data-analysis",
    "href": "projects/telecom_churn/churn_telecom.html#exploratory-data-analysis",
    "title": "Telecom Customer Churn Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nCheck Cardinality\n\n#Check State column \nchurn_data %&gt;% \n  count(State, sort = TRUE)\n\n# A tibble: 51 × 2\n   State     n\n   &lt;chr&gt; &lt;int&gt;\n 1 WV      106\n 2 MN       84\n 3 NY       83\n 4 AL       80\n 5 OH       78\n 6 OR       78\n 7 WI       78\n 8 VA       77\n 9 WY       77\n10 CT       74\n# ℹ 41 more rows\n\n\n\n#Check International Plan and Voice Mail Plan\nchurn_data %&gt;% \n  count(`International plan`,sort = TRUE)\n\n# A tibble: 2 × 2\n  `International plan`     n\n  &lt;chr&gt;                &lt;int&gt;\n1 No                    3010\n2 Yes                    323\n\nchurn_data %&gt;% \n  count(`Voice mail plan`,sort = TRUE)\n\n# A tibble: 2 × 2\n  `Voice mail plan`     n\n  &lt;chr&gt;             &lt;int&gt;\n1 No                 2411\n2 Yes                 922\n\n\nMajority of customers don’t have an International and Voice Mail plan\n\nchurn_data %&gt;% \n  filter(`Number vmail messages` != 0) %&gt;% \n  ggplot(aes(x = `Number vmail messages`)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFor the number of vmail messages among those customers that has a Voice mail plan, we can see that most of the count are concentrated on 30 voice mail messages. I filtered out those customers with no voice mail plan to avoid underestimation of our histogram for the customers that has the voice mail plan.\nThe next numerical columns we will explore are all measure of how many minutes that customer spent for calling during the day, eve, night and international call if the customer has an international call plan.\nLet’s visualize the distribution for day usage.\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total day minutes`)) +\n  geom_histogram()\n\n\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total day calls`)) +\n  geom_histogram()\n\n\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total day charge`)) +\n  geom_histogram()\n\n\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total day minutes`,y = `Total day charge`)) +\n  geom_point()\n\n\n\n\nAlmost perfect linear relationship between charge and minutes. This is expected to happen as we will be charge more the longer we use the phone for calls.\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total day calls`, y = `Total day minutes`)) +\n  geom_point()\n\n\n\n\nTotal Day Calls column seems to have a different relationship with minutes. This kind of make sense, because the number of calls you make per day doesn’t really mean that you will have higher minutes of call. One can make 3 calls in day where in each call lasts for 30 minutes, while another person can make 10 phone calls with each call lasting for only 1 minute.\nLet’s try to associate our target variable to see what is the difference of Total Day distribution for Customers that Churned and for those who did not.\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total day calls`,fill = Churn)) +\n  geom_histogram()\n\n\n\n\nAs expected, customers that are not using the service for phone calls will tend to just end their subscription. Let’s see if this will be the same pattern for minutes and charge.\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total day charge`, fill = Churn)) +\n  geom_histogram()\n\n\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total day minutes`, fill = Churn)) +\n  geom_histogram()\n\n\n\n\nAs expected, minutes and charge exhibits the same behavior for Customers that Churned and did not. Next, let’s take a look on eve charge, minutes and calls.\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total eve calls`))+\n  geom_histogram()\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total eve calls`, fill = Churn))+\n  geom_histogram()\n\n\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total eve charge`))+\n  geom_histogram()\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total eve charge`, fill = Churn))+\n  geom_histogram()\n\n\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total eve minutes`))+\n  geom_histogram()\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total eve minutes`, fill = Churn))+\n  geom_histogram()\n\n\n\n\nAs we can see from the visuals above, the distribution with and without filter follows the same pattern from the day calls, minutes and charge. Lets continue to explore the distribution for night and international minutes, calls and charge\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total night calls`, fill = Churn))+\n  geom_histogram()\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total night minutes`, fill = Churn))+\n  geom_histogram()\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total night charge`, fill = Churn))+\n  geom_histogram()\n\n\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total intl calls`, fill = Churn)) +\n  geom_histogram()\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total intl minutes`, fill = Churn)) +\n  geom_histogram()\n\n\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Total intl charge`, fill = Churn)) +\n  geom_histogram()\n\n\n\n\nTotal International calls is different from the rest of distribution since it is skewed. Finally, let’s check our service calls column.\n\nchurn_data %&gt;% \n  ggplot(aes(x = `Customer service calls`, fill = Churn)) +\n  geom_histogram()"
  },
  {
    "objectID": "projects/telecom_churn/churn_telecom.html#data-split",
    "href": "projects/telecom_churn/churn_telecom.html#data-split",
    "title": "Telecom Customer Churn Analysis",
    "section": "Data Split",
    "text": "Data Split\nNow that we have some assumptions from the EDA we did, we can start preparing our data by splitting it into test and train data set. This is a vital part of any machine learning model project. This will allow our model to learn the pattern of our data and use that to predict unseen data coming from our test data set.\n\nset.seed(222)\n\n#Lets Remove Account Length and Area code from our data set\nchurn_data_model = churn_data %&gt;% \n  select(-c(`Account length`,`Area code`)) %&gt;% \n  mutate(\n    Churn = ifelse(Churn,1,0) %&gt;% as.factor()\n  )\n\ndata_split = initial_split(churn_data_model, prop = 3/4)\ntrain_data = training(data_split)\ntest_data = testing(data_split)\n\nThat’s it! We now have 2 sets of data for our modelling. Now we need to create a recipe for basic logistic regression."
  },
  {
    "objectID": "projects/telecom_churn/churn_telecom.html#data-modelling",
    "href": "projects/telecom_churn/churn_telecom.html#data-modelling",
    "title": "Telecom Customer Churn Analysis",
    "section": "Data Modelling",
    "text": "Data Modelling\nFor tidymodels ecosystem, the first step in data modelling is creating your model recipe. Here in our case, we will predict Churn based on all available variables using our train data. Then we will initialize our model engine which GLM since we will use Logistic Regression for our base model. Once we are done with our recipe and engine model, we will add those 2 on our workflow to prepare it for the actual model fitting.\n\n#Create Recipe\nchurn_recipe = recipe(Churn ~ ., data = train_data)\n\nlogit_model = logistic_reg() %&gt;% \n  set_engine(\"glm\")\n\nchurn_workflow = workflow() %&gt;% \n  add_model(logit_model) %&gt;% \n  add_recipe(churn_recipe)\n\nchurn_fit = churn_workflow %&gt;% \n  fit(data = train_data)\n\nWe can check the result of our base model by using the function extract_fit_parsnip and tidy.\n\nchurn_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  tidy() %&gt;% \n  filter(p.value &lt; 0.05)\n\n# A tibble: 9 × 5\n  term                     estimate std.error statistic  p.value\n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)               -9.76      1.06       -9.24 2.53e-20\n2 StateCA                    1.78      0.830       2.14 3.21e- 2\n3 StateME                    1.57      0.758       2.08 3.79e- 2\n4 StateMT                    1.79      0.743       2.41 1.60e- 2\n5 StateNJ                    1.50      0.730       2.05 4.04e- 2\n6 `International plan`Yes    2.23      0.176      12.7  6.56e-37\n7 `Voice mail plan`Yes      -1.92      0.708      -2.71 6.81e- 3\n8 `Total intl calls`        -0.0841    0.0299     -2.82 4.84e- 3\n9 `Customer service calls`   0.502     0.0473     10.6  2.29e-26"
  },
  {
    "objectID": "projects/telecom_churn/churn_telecom.html#data-prediction-and-evaluation",
    "href": "projects/telecom_churn/churn_telecom.html#data-prediction-and-evaluation",
    "title": "Telecom Customer Churn Analysis",
    "section": "Data Prediction and Evaluation",
    "text": "Data Prediction and Evaluation\nNow that we have our model trained, we can proceed on predicting unseen data using our test data set.\n\npredict(churn_fit, test_data) %&gt;% \n  count(.pred_class)\n\n# A tibble: 2 × 2\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0             788\n2 1              46\n\n\nOur Model predicted 788 customers that did not churn and 46 that did churn. We can view it another format where we can check the predicted class probabilities for the Churn column.\n\nchurn_aug = augment(churn_fit,test_data)\n\nchurn_aug %&gt;% \n  head()\n\n# A tibble: 6 × 21\n  .pred_class .pred_0 .pred_1 State `International plan` `Voice mail plan`\n  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;            \n1 0             0.978  0.0217 HI    No                   No               \n2 0             0.763  0.237  ID    No                   No               \n3 0             0.962  0.0383 KY    No                   No               \n4 0             0.941  0.0594 LA    No                   No               \n5 0             0.918  0.0825 NM    No                   No               \n6 0             0.915  0.0855 IN    No                   No               \n# ℹ 15 more variables: `Number vmail messages` &lt;int&gt;,\n#   `Total day minutes` &lt;dbl&gt;, `Total day calls` &lt;int&gt;,\n#   `Total day charge` &lt;dbl&gt;, `Total eve minutes` &lt;dbl&gt;,\n#   `Total eve calls` &lt;int&gt;, `Total eve charge` &lt;dbl&gt;,\n#   `Total night minutes` &lt;dbl&gt;, `Total night calls` &lt;int&gt;,\n#   `Total night charge` &lt;dbl&gt;, `Total intl minutes` &lt;dbl&gt;,\n#   `Total intl calls` &lt;int&gt;, `Total intl charge` &lt;dbl&gt;, …\n\n\nTo start our model evaluation, Let’s generate an ROC curve.\n\nchurn_aug %&gt;% \n  roc_curve(truth = Churn,.pred_0) %&gt;% \n  autoplot()\n\n\n\n\nCalculate Accuracy of our model\n\nchurn_testing_pred = predict(churn_fit, test_data) %&gt;% \n  bind_cols(predict(churn_fit, test_data, type = \"prob\")) %&gt;% \n  bind_cols(test_data %&gt;% select(Churn))\n\nchurn_testing_pred\n\n# A tibble: 834 × 4\n   .pred_class .pred_0 .pred_1 Churn\n   &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;\n 1 0             0.978  0.0217 0    \n 2 0             0.763  0.237  1    \n 3 0             0.962  0.0383 0    \n 4 0             0.941  0.0594 0    \n 5 0             0.918  0.0825 0    \n 6 0             0.915  0.0855 0    \n 7 0             0.969  0.0315 0    \n 8 0             0.774  0.226  0    \n 9 0             0.950  0.0498 0    \n10 0             0.904  0.0957 0    \n# ℹ 824 more rows\n\n\n\nchurn_testing_pred %&gt;% \n  accuracy(truth = Churn, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.865\n\n\nNot bad! We got an accuracy rate of 86% for base model without even doing extensive data pre-processing. But what we did from the script above is a basic data split and modelling. Now let’s try enhancing our data split process by doing a K-Fold Cross Validation and see if it will help our model to perform better.\n\nset.seed(345)\n\nfolds = vfold_cv(train_data, v = 10)\nfolds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [2249/250]&gt; Fold01\n 2 &lt;split [2249/250]&gt; Fold02\n 3 &lt;split [2249/250]&gt; Fold03\n 4 &lt;split [2249/250]&gt; Fold04\n 5 &lt;split [2249/250]&gt; Fold05\n 6 &lt;split [2249/250]&gt; Fold06\n 7 &lt;split [2249/250]&gt; Fold07\n 8 &lt;split [2249/250]&gt; Fold08\n 9 &lt;split [2249/250]&gt; Fold09\n10 &lt;split [2250/249]&gt; Fold10\n\n\nLets create a new workflow for this approach\n\nchurn_Cv_wf = workflow() %&gt;% \n  add_model(logit_model) %&gt;% \n  add_formula(Churn ~ .) \n\nset.seed(456)\n\nchurn_cv_fit = churn_Cv_wf %&gt;% \n  fit_resamples(folds)\n\ncollect_metrics(churn_cv_fit)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.858    10 0.00900 Preprocessor1_Model1\n2 roc_auc  binary     0.796    10 0.0121  Preprocessor1_Model1\n\n\nThe result above shows that the performance of our training model is not that far on how our model performs on predicting unseen data from our test data set. This is a good evidence that our training data is not overfitting or capturing noise from our training data set and setting it as a pattern to learn."
  },
  {
    "objectID": "projects/telecom_churn/churn_telecom.html#decision-tree-model-with-k-cross-fold-validation",
    "href": "projects/telecom_churn/churn_telecom.html#decision-tree-model-with-k-cross-fold-validation",
    "title": "Telecom Customer Churn Analysis",
    "section": "Decision Tree Model with K-Cross Fold Validation",
    "text": "Decision Tree Model with K-Cross Fold Validation\nIt is good to know that using only our base model, we were able to get an accuracy of 86%, but we wont stop on that point. It is always recommended to check at least 3 modelling algorithm for comparison. For our 2nd model algorithm, we will use Decision Tree with k-Cross Fold Validation.\n\nData Split\n\nset.seed(1234)\ncolnames(churn_data_model) = str_replace_all(colnames(churn_data_model),\" \",\"_\") \n\ndtree_split = initial_split(churn_data_model)\ndtree_train = training(dtree_split)\ndtree_test = testing(dtree_split)\n\n#Setting up fold value\ndtree_fold = vfold_cv(dtree_train,v = 10)\n\n\n\nModel Recipe and Engine Initialization\n\ndtree_recipe = recipe(Churn ~ ., data= dtree_train) %&gt;% \n  step_normalize(all_numeric())\n\ntree_model = decision_tree() %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"classification\")\n\n\n\nWorkflow and Model Fitting\n\ndtree_wf = workflow() %&gt;% \n  add_model(tree_model) %&gt;% \n  add_recipe(dtree_recipe)\n\nset.seed(1120)\ndtree_churn_cv_fit = dtree_wf %&gt;% \n  fit_resamples(dtree_fold)\n\ncollect_metrics(dtree_churn_cv_fit)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.922    10 0.00397 Preprocessor1_Model1\n2 roc_auc  binary     0.885    10 0.00903 Preprocessor1_Model1\n\n\nSo for our training model, our average Accuracy is at 92%. Impressive performance on our train data set. Let’s check if what will be the performance of our model on test data set.\n\n\nModel Evaluation\n\ndtree_fit = dtree_wf %&gt;% \n  fit(data = dtree_train)\n\ndtree_aug = augment(dtree_fit,dtree_test)\n\ndtree_aug %&gt;% \n  head()\n\n# A tibble: 6 × 21\n  .pred_class .pred_0 .pred_1 State International_plan Voice_mail_plan\n  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;              &lt;chr&gt;          \n1 0             0.972  0.0276 LA    No                 No             \n2 0             0.972  0.0276 VA    No                 No             \n3 0             0.972  0.0276 VT    No                 Yes            \n4 0             0.972  0.0276 MA    No                 No             \n5 0             0.972  0.0276 OR    No                 No             \n6 0             0.972  0.0276 WY    No                 No             \n# ℹ 15 more variables: Number_vmail_messages &lt;int&gt;, Total_day_minutes &lt;dbl&gt;,\n#   Total_day_calls &lt;int&gt;, Total_day_charge &lt;dbl&gt;, Total_eve_minutes &lt;dbl&gt;,\n#   Total_eve_calls &lt;int&gt;, Total_eve_charge &lt;dbl&gt;, Total_night_minutes &lt;dbl&gt;,\n#   Total_night_calls &lt;int&gt;, Total_night_charge &lt;dbl&gt;,\n#   Total_intl_minutes &lt;dbl&gt;, Total_intl_calls &lt;int&gt;, Total_intl_charge &lt;dbl&gt;,\n#   Customer_service_calls &lt;int&gt;, Churn &lt;fct&gt;\n\n\nLet’s also take a look on the selected important features of our data.\n\ntree_plot = dtree_fit %&gt;% \n  pull_workflow_fit() \nrpart.plot(tree_plot$fit)\n\n\n\n\n\ndtree_aug %&gt;% \n  roc_curve(truth = Churn,.pred_0) %&gt;% \n  autoplot()\n\n\n\ndtree_aug %&gt;% \n  roc_curve(truth = Churn,.pred_1) %&gt;% \n  autoplot()\n\n\n\n\n\ndtree_testing_pred = predict(dtree_fit, dtree_test) %&gt;% \n  bind_cols(predict(dtree_fit, dtree_test, type = \"prob\")) %&gt;% \n  bind_cols(dtree_test %&gt;% select(Churn))\n\ndtree_testing_pred\n\n# A tibble: 834 × 4\n   .pred_class .pred_0 .pred_1 Churn\n   &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;\n 1 0             0.972  0.0276 0    \n 2 0             0.972  0.0276 0    \n 3 0             0.972  0.0276 0    \n 4 0             0.972  0.0276 0    \n 5 0             0.972  0.0276 0    \n 6 0             0.972  0.0276 0    \n 7 0             0.843  0.157  0    \n 8 0             0.972  0.0276 0    \n 9 0             0.972  0.0276 0    \n10 0             0.972  0.0276 1    \n# ℹ 824 more rows\n\ndtree_testing_pred %&gt;% \n  accuracy(truth = Churn, .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.927\n\n\nAmazing! We now have a better performing model by using Decision Tree! Let’s try to add one last model. Who know, maybe we can even reach an accuracy rate of 95% right? On our next model we will use XGBoost."
  },
  {
    "objectID": "projects/telecom_churn/churn_telecom.html#xgboost-modelling-with-cross-fold-validation",
    "href": "projects/telecom_churn/churn_telecom.html#xgboost-modelling-with-cross-fold-validation",
    "title": "Telecom Customer Churn Analysis",
    "section": "XGBoost Modelling with Cross-Fold Validation",
    "text": "XGBoost Modelling with Cross-Fold Validation\n\nData Split\n\nxg_split = initial_split(\n  churn_data_model,\n  prop = 3/4,\n  strata = Churn\n)\n\nxg_train = training(xg_split)\nxg_test = testing(xg_split)\n\nxg_cv_fold = vfold_cv(xg_train, v = 10)\n\nxg_recipe = recipe( Churn ~ ., data = xg_train) %&gt;% \n  step_normalize(all_numeric()) %&gt;% \n  prep()\n\n\n\nXGBoost Model Specification\n\n#XGBoost Model Specs\nxgboost_model = boost_tree(\n  mode = \"classification\",\n  trees = 100,\n  min_n = tune(),\n  tree_depth = tune(),\n  learn_rate = tune(),\n  loss_reduction = tune()\n) %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"classification\")\n\n#Grid Specification for Parameters\nxgboos_params = dials::parameters(\n  min_n(),\n    tree_depth(),\n    learn_rate(),\n    loss_reduction()\n)\n\n#Set up Grid Space\nxgboost_grid = dials::grid_max_entropy(\n  xgboos_params,\n  size = 60\n)\nknitr::kable(head(xgboost_grid))\n\n\n\n\nmin_n\ntree_depth\nlearn_rate\nloss_reduction\n\n\n\n\n5\n4\n0.0024475\n4.9402795\n\n\n16\n10\n0.0000000\n0.0000000\n\n\n7\n6\n0.0000010\n0.0000000\n\n\n22\n7\n0.0000000\n0.0000000\n\n\n6\n12\n0.0000001\n0.0000000\n\n\n11\n8\n0.0061443\n0.0000146\n\n\n\n\n\n\n\nDefine Worklow\n\nxgboost_wf = workflow() %&gt;% \n  add_model(xgboost_model) %&gt;% \n  add_formula(Churn ~ .)\n\n\n\nTune the Model\n\nxgboost_tuned = tune_grid(\n  object = xgboost_wf,\n  resamples = xg_cv_fold,\n  grid = xgboost_grid,\n  metrics = metric_set(accuracy, roc_auc),\n  control = control_grid(verbose = TRUE)\n)\n\nNow that we are done with Model Tuning, let’s explore the result of the object.\n\nxgboost_tuned %&gt;% \n  tune::show_best(metric = \"accuracy\")\n\n# A tibble: 5 × 10\n  min_n tree_depth   learn_rate loss_reduction .metric  .estimator  mean     n\n  &lt;int&gt;      &lt;int&gt;        &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;\n1     3         11 0.00110            8.83e- 1 accuracy binary     0.938    10\n2    19          5 0.0632             7.00e-10 accuracy binary     0.937    10\n3     4          8 0.000107           1.31e- 5 accuracy binary     0.934    10\n4     6         12 0.0000000980       5.69e- 9 accuracy binary     0.928    10\n5     7          6 0.000000984        9.60e-10 accuracy binary     0.926    10\n# ℹ 2 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt;\n\n\nNext, we will isolate the best performing hyperparameter values.\n\nxgboost_best_params = xgboost_tuned %&gt;% \n  select_best(\"accuracy\")\n\nxgboost_best_params\n\n# A tibble: 1 × 5\n  min_n tree_depth learn_rate loss_reduction .config              \n  &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;                \n1     3         11    0.00110          0.883 Preprocessor1_Model13\n\n\nFinalize the XGBoost model to use the best tuning parameters.\n\nxgboost_model_final = xgboost_model %&gt;% \n  finalize_model(xgboost_best_params)\n\nNow that we have our final model, let’s test it using our training model first. This is a recommended way of checking if our model is overfitting.\n\ntrain_processed = bake(xg_recipe, new_data = xg_train)\n\ntrain_prediction = xgboost_model_final %&gt;% \n  fit(\n    formula = Churn ~ .,\n    data = train_processed\n  ) %&gt;% \n  predict(new_data = train_processed) %&gt;% \n  bind_cols(xg_train)\n\nxgboost_score_train = train_prediction %&gt;% \n  metrics(Churn, .pred_class)\n\n\nxgboost_score_train\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.953\n2 kap      binary         0.798\n\n\nWe have 96% accuracy for training data. Let’s check the performance of our model on our testing data.\n\ntest_process = bake(xg_recipe, new_data = xg_test)\n\ntest_prediction = xgboost_model_final %&gt;% \n  fit(\n    formula = Churn ~ ., \n    data = test_process\n  ) %&gt;% \n  predict(new_data = test_process) %&gt;% \n  bind_cols(xg_test)\n\n\n\nxgboost_score_test = test_prediction %&gt;% \n  metrics(Churn, .pred_class)\n\n\nxgboost_score_test\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.923\n2 kap      binary         0.647\n\n\nXGBoost model resulted to an almost same performing model with the Decision Tree model. We can either select XGboost or Decision Tree for our Final model. For me, I will just settle with Decision Tree. We will still have the 92% accurate model with a simple model implementation and for that I think Decision Tree is a good choice."
  }
]